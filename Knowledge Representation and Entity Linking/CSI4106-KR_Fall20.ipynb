{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 8 - Knowledge Representation (KR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSI4106 Artificial Intelligence   \n",
    "Fall 2020  \n",
    "Prepared by Julian Templeton and Caroline Barrière"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***INTRODUCTION***:  \n",
    "\n",
    "When reading text, understanding the type of entities with the text helps extract additional information about the entity. Through the use of Named Entity Recognition (NER), we are able to determine whether an entity is a Person, Organization, Country, ... When exploring text online, we also occassionally see entities have clickable links to webpages with more information on the entity. This is a form of enhancing the text to allow readers to easily access the information needed to understand each entity from the text and its content.    \n",
    "\n",
    "In this notebook we will be revisiting the Covid-19 related news dataset from notebook 7 to explore how we can improve spaCy's NER disambiguation and enhance the text from the news articles through the use of entity linking. This will be done in two parts, where we first use text coherence for NER disambiguation and then perform text enhancement with entity linking.    \n",
    "\n",
    "**For this notebook, do not modify function definitions and be sure to use the setup that is provided to you AND only submit this file, nothing else is needed**.\n",
    "\n",
    "This notebook uses libraries that have been used in previous notebooks, including spaCy and pandas. Recall that if you run into any issues with loading 'en' to comment that line and uncomment the included line of code (the same way that you may have done in notebook 7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***HOMEWORK***:  \n",
    "Go through the notebook by running each cell, one at a time.  \n",
    "Look for **(TO DO)** for the tasks that you need to perform. Do not edit the code outside of the questions which you are asked to answer unless specifically asked. Once you're done, Sign the notebook (at the end of the notebook), and submit it.  \n",
    "\n",
    "*The notebook will be marked on 30.  \n",
    "Each **(TO DO)** has a number of points associated with it.*\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before starting we will import every module that we will be using\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The core spacy object that will be used for tokenization, lemmatization, POS Tagging, ...\n",
    "# Note that this is specifically for the English language and requires the English package to be installed\n",
    "# via pip to work as intended.\n",
    "\n",
    "#sp = spacy.load('en')\n",
    "\n",
    "# If the above causes an error after installing the package described in (2), install the package described\n",
    "# in the Note section within the introduction and run this line of code instead of the above.\n",
    "sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 1 - Text Coherence for NER Disambiguation**  \n",
    "  \n",
    "For the first part of this notebook we will use the modules from *spaCy* to help to perform NER disambiguation and better the results with the text coherence on documents from the included file on Covid-19 related news articles from CBC news (the same file from notebook 7). We will begin by looking at the NER disambiguation that is performed by spaCy and think of some simple methods to use the coherence of the entities within the text to potentially improve the NER disambiguation.   \n",
    "\n",
    "\n",
    "As with last notebook, the dataset is included with this notebook, but details regarding it can be found [here](https://www.kaggle.com/ryanxjhan/cbc-news-coronavirus-articles-march-26?select=news.csv). The first thing that we will do, as usual, is load the file into a pandas dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>'More vital now:' Gay-straight alliances go vi...</td>\n",
       "      <td>2020-05-03 1:30</td>\n",
       "      <td>Lily Overacker and Laurell Pallot start each g...</td>\n",
       "      <td>Lily Overacker and Laurell Pallot start each g...</td>\n",
       "      <td>https://www.cbc.ca/news/canada/calgary/gay-str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Scientists aim to 'see' invisible transmission...</td>\n",
       "      <td>2020-05-02 8:00</td>\n",
       "      <td>Some researchers aim to learn more about how t...</td>\n",
       "      <td>This is an excerpt from Second Opinion, a week...</td>\n",
       "      <td>https://www.cbc.ca/news/technology/droplet-tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>['The Canadian Press']</td>\n",
       "      <td>Coronavirus: What's happening in Canada and ar...</td>\n",
       "      <td>2020-05-02 11:28</td>\n",
       "      <td>Canada's chief public health officer struck an...</td>\n",
       "      <td>The latest:  The lives behind the numbers: Wha...</td>\n",
       "      <td>https://www.cbc.ca/news/canada/coronavirus-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>B.C. announces 26 new coronavirus cases, new c...</td>\n",
       "      <td>2020-05-02 18:45</td>\n",
       "      <td>B.C. provincial health officer Dr. Bonnie Henr...</td>\n",
       "      <td>B.C. provincial health officer Dr. Bonnie Henr...</td>\n",
       "      <td>https://www.cbc.ca/news/canada/british-columbi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "      <td>B.C. announces 26 new coronavirus cases, new c...</td>\n",
       "      <td>2020-05-02 18:45</td>\n",
       "      <td>B.C. provincial health officer Dr. Bonnie Henr...</td>\n",
       "      <td>B.C. provincial health officer Dr. Bonnie Henr...</td>\n",
       "      <td>https://www.cbc.ca/news/canada/british-columbi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>['Senior Writer', 'Chris Arsenault Is A Senior...</td>\n",
       "      <td>Brazil has the most confirmed COVID-19 cases i...</td>\n",
       "      <td>2020-05-02 8:00</td>\n",
       "      <td>From describing coronavirus as a \"little flu,\"...</td>\n",
       "      <td>With infection rates spiralling, some big city...</td>\n",
       "      <td>https://www.cbc.ca/news/world/brazil-has-the-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>['Cbc News']</td>\n",
       "      <td>The latest on the coronavirus outbreak for May 1</td>\n",
       "      <td>2020-05-01 20:43</td>\n",
       "      <td>The latest on the coronavirus outbreak from CB...</td>\n",
       "      <td>Coronavirus Brief (CBC)  Canada is officiall...</td>\n",
       "      <td>https://www.cbc.ca/news/the-latest-on-the-coro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>['Cbc News']</td>\n",
       "      <td>Coronavirus: What's happening in Canada and ar...</td>\n",
       "      <td>2020-05-01 11:51</td>\n",
       "      <td>Nova Scotia announced Friday it is immediately...</td>\n",
       "      <td>The latest:  The lives behind the numbers: Wha...</td>\n",
       "      <td>https://www.cbc.ca/news/canada/coronavirus-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>['Senior Writer', \"Adam Miller Is Senior Digit...</td>\n",
       "      <td>Did the WHO mishandle the global coronavirus p...</td>\n",
       "      <td>2020-04-30 8:00</td>\n",
       "      <td>The World Health Organization has come under f...</td>\n",
       "      <td>The World Health Organization has come under f...</td>\n",
       "      <td>https://www.cbc.ca/news/health/coronavirus-who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>['Thomson Reuters']</td>\n",
       "      <td>Armed people in Michigan's legislature protest...</td>\n",
       "      <td>2020-04-30 21:37</td>\n",
       "      <td>Hundreds of protesters, some armed, gathered a...</td>\n",
       "      <td>Hundreds of protesters, some armed, gathered a...</td>\n",
       "      <td>https://www.cbc.ca/news/world/protesters-michi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                                            authors  \\\n",
       "0          0                                                 []   \n",
       "1          1                                                 []   \n",
       "2          2                             ['The Canadian Press']   \n",
       "3          3                                                 []   \n",
       "4          4                                                 []   \n",
       "5          5  ['Senior Writer', 'Chris Arsenault Is A Senior...   \n",
       "6          6                                       ['Cbc News']   \n",
       "7          7                                       ['Cbc News']   \n",
       "8          8  ['Senior Writer', \"Adam Miller Is Senior Digit...   \n",
       "9          9                                ['Thomson Reuters']   \n",
       "\n",
       "                                               title      publish_date  \\\n",
       "0  'More vital now:' Gay-straight alliances go vi...   2020-05-03 1:30   \n",
       "1  Scientists aim to 'see' invisible transmission...   2020-05-02 8:00   \n",
       "2  Coronavirus: What's happening in Canada and ar...  2020-05-02 11:28   \n",
       "3  B.C. announces 26 new coronavirus cases, new c...  2020-05-02 18:45   \n",
       "4  B.C. announces 26 new coronavirus cases, new c...  2020-05-02 18:45   \n",
       "5  Brazil has the most confirmed COVID-19 cases i...   2020-05-02 8:00   \n",
       "6   The latest on the coronavirus outbreak for May 1  2020-05-01 20:43   \n",
       "7  Coronavirus: What's happening in Canada and ar...  2020-05-01 11:51   \n",
       "8  Did the WHO mishandle the global coronavirus p...   2020-04-30 8:00   \n",
       "9  Armed people in Michigan's legislature protest...  2020-04-30 21:37   \n",
       "\n",
       "                                         description  \\\n",
       "0  Lily Overacker and Laurell Pallot start each g...   \n",
       "1  Some researchers aim to learn more about how t...   \n",
       "2  Canada's chief public health officer struck an...   \n",
       "3  B.C. provincial health officer Dr. Bonnie Henr...   \n",
       "4  B.C. provincial health officer Dr. Bonnie Henr...   \n",
       "5  From describing coronavirus as a \"little flu,\"...   \n",
       "6  The latest on the coronavirus outbreak from CB...   \n",
       "7  Nova Scotia announced Friday it is immediately...   \n",
       "8  The World Health Organization has come under f...   \n",
       "9  Hundreds of protesters, some armed, gathered a...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Lily Overacker and Laurell Pallot start each g...   \n",
       "1  This is an excerpt from Second Opinion, a week...   \n",
       "2  The latest:  The lives behind the numbers: Wha...   \n",
       "3  B.C. provincial health officer Dr. Bonnie Henr...   \n",
       "4  B.C. provincial health officer Dr. Bonnie Henr...   \n",
       "5  With infection rates spiralling, some big city...   \n",
       "6    Coronavirus Brief (CBC)  Canada is officiall...   \n",
       "7  The latest:  The lives behind the numbers: Wha...   \n",
       "8  The World Health Organization has come under f...   \n",
       "9  Hundreds of protesters, some armed, gathered a...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.cbc.ca/news/canada/calgary/gay-str...  \n",
       "1  https://www.cbc.ca/news/technology/droplet-tra...  \n",
       "2  https://www.cbc.ca/news/canada/coronavirus-cov...  \n",
       "3  https://www.cbc.ca/news/canada/british-columbi...  \n",
       "4  https://www.cbc.ca/news/canada/british-columbi...  \n",
       "5  https://www.cbc.ca/news/world/brazil-has-the-m...  \n",
       "6  https://www.cbc.ca/news/the-latest-on-the-coro...  \n",
       "7  https://www.cbc.ca/news/canada/coronavirus-cov...  \n",
       "8  https://www.cbc.ca/news/health/coronavirus-who...  \n",
       "9  https://www.cbc.ca/news/world/protesters-michi...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset, show top ten rows\n",
    "df = pd.read_csv(\"news.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, when we explored how spaCy can perform the various steps of the NLP pipeline, we saw that it was able to perform Named Entity Recognition (NER). Below is the same example that we saw from the last notebook to showcase how we can access spaCy's NER type predictions for tokens in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Canada\" is a GPE\n",
      "\"at least two metres\" is a QUANTITY\n"
     ]
    }
   ],
   "source": [
    "# Same example from notebook 7, recall that we loop through the iterator found in the .ents property of a parsed sentence\n",
    "sentence_example = \"Government guidelines in Canada recommend that people stay at least two metres away from others as part of physical distancing measures to curb the spread of COVID-19.\"\n",
    "sentence_example_content = sp(sentence_example)\n",
    "# Loop through all tokens that contain a NER type and print the token along with the corresponding NER type\n",
    "for token in sentence_example_content.ents:\n",
    "    print(\"\\\"\" + token.text + \"\\\" is a \" + token.label_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q1**  \n",
    "Before performing NER with text coherence, you will first explore how spaCy performs NER disambiguation. In the text of ***second document*** (index 1) of our corpus of documents, which words are *PER* (spaCy uses the *PERSON* type, rather than *PER*), *ORG* (Organiztion), and *GPE* (Geopolitical Entity). You must do the following for this question:    \n",
    "a) Print each *PER*, *ORG*, and *GPE* along with its NER type from spaCy.     \n",
    "b) Are all of these NER type predictions correct? If not, provide three examples of incorrect outputs.    \n",
    "c) Do any of the problems with the NER type predictions come from an earlier step in the NLP pipeline that is performed by spaCy? Describe the problem for two examples from the output above.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q1 (a) - 2 marks**  \n",
    "Print each *PER*, *ORG*, and *GPE* along with its NER type from spaCy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the World Health Organization\" is a ORG\n",
      "\"Touches\" is a ORG\n",
      "\"WHO\" is a ORG\n",
      "\"the Public Health Agency\" is a ORG\n",
      "\"W.F. Wells\" is a PERSON\n",
      "\"Harvard School of Public Health\" is a ORG\n",
      "\"Wells\" is a ORG\n",
      "\"Canada\" is a GPE\n",
      "\"Lydia Bourouiba\" is a PERSON\n",
      "\"the Fluid Dynamics of Disease Transmission Laboratory\" is a ORG\n",
      "\"the Massachusetts Institute of Technology\" is a ORG\n",
      "\"Bourouiba\" is a PERSON\n",
      "\"Mark Loeb\" is a PERSON\n",
      "\"Hamilton\" is a PERSON\n",
      "\"McMaster University\" is a ORG\n",
      "\"RNA\" is a GPE\n",
      "\"Wuhan\" is a GPE\n",
      "\"China\" is a GPE\n",
      "\"Nebraska\" is a GPE\n",
      "\"Loeb\" is a PERSON\n",
      "\"Loeb\" is a PERSON\n",
      "\"Canada\" is a GPE\n",
      "\"Gary Moore/CBC\" is a PERSON\n",
      "\"Allison McGeer\" is a PERSON\n",
      "\"Sinai Health\" is a ORG\n",
      "\"Toronto\" is a GPE\n",
      "\"particles \" is a PERSON\n",
      "\"McGeer\" is a ORG\n",
      "\"McGeer\" is a PERSON\n",
      "\"Bourouiba\" is a GPE\n",
      "\"Bourouiba\" is a PERSON\n",
      "\"Bourouiba/MIT/\" is a ORG\n",
      "\"Samira Mubareka\" is a PERSON\n",
      "\"Sunnybrook Hospital\" is a ORG\n",
      "\"Toronto\" is a GPE\n",
      "\"Bourouiba\" is a PERSON\n",
      "\"JAMA Insights\" is a ORG\n",
      "\"McMaster\" is a PERSON\n",
      "\"Loeb\" is a PERSON\n",
      "\"N95\" is a ORG\n",
      "\"U.S.\" is a GPE\n",
      "\"Justin Trudeau\" is a PERSON\n",
      "\"the New England Journal of Medicine\" is a ORG\n",
      "\"the U.S. National Institutes of Health\" is a ORG\n",
      "\"U.S. National Institutes of Health\" is a ORG\n",
      "\"Journal of the Royal Society Interface\" is a ORG\n",
      "\"U.S.\" is a GPE\n",
      "\"Singapore\" is a GPE\n",
      "\"N95\" is a ORG\n",
      "\"Gary S. Settles\" is a PERSON\n",
      "\"Penn State University/Journal of the Royal Society Interface\" is a ORG\n",
      "\"The World Health Organization\" is a ORG\n",
      "\"Los Angeles\" is a GPE\n",
      "\"Italy\" is a GPE\n",
      "\"Austria\" is a GPE\n"
     ]
    }
   ],
   "source": [
    "# Select the second document (index 1)\n",
    "doc = df[\"text\"][1]\n",
    "# TODO\n",
    "docSP = sp(doc)\n",
    "\n",
    "for token in docSP.ents:\n",
    "    if(token.label_ == 'PERSON' or token.label_ == 'ORG' or token.label_== 'GPE' ):\n",
    "        print(\"\\\"\" + token.text + \"\\\" is a \" + token.label_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q1 (b) - 1 mark**   \n",
    "Are all of these NER type predictions correct? If not, provide two examples of incorrect outputs.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO ...  \n",
    "\"Touches\" is a ORG\n",
    "Not an organization\n",
    "\"RNA\" is a GPE\n",
    "Not a place\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q1 (c) - 2 marks**   \n",
    "Do any of the problems with the NER type predictions come from an earlier step in the NLP pipeline that is performed by spaCy? Describe the problem for two examples from the output above.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO ... \n",
    "The problems can occur depending on the way spacy does the POS tagging, for example if it lists a word which isn't a noun, as a noun the NER type prediction is likely affected. That is what might have happened with \"Touches\" being listed as an organization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you saw that spaCy NER does not always perform correctly, we will try to use text coherence to modify the NER types that spaCy gave.  In fact spaCy assigns the entity types one sentence at a time.  But when looking a the whole document, and knowing that text is usually coherent, we can do some post-processing to spaCy's NER module and correct some mistakes.  By text being coherent, we mean, for example, that if a person is referred to with a particular name, e.g. *McGeer*, chances are that each time we see *McGeer* in the document, it is the same person.  So it is unlikely that *McGeer* would be once a person and once an organization.  It is not always true, but it is a common assumption.  Therefore, we will explore two different strategies to use text coherence to post-process the output from the spaCy NER module.  \n",
    "\n",
    "The first strategy (*explored in Q2/Q3*) is to find, among all NER types assigned, which is the most frequent one.  For example, the entity *Bourouiba* was assigned 1 time GPE, and 3 times PERSON, so this information can be used to modify the GPE type and change it to PERSON.  \n",
    "\n",
    "The second strategy (explored in Q4) is to try to find a longer form in the text.  Since that longer form should be less ambiguous, we can use it to disambiguate the shorter, more ambiguous forms.  For example, *Lydia Bourouiba* occurs in the text and is assigned PERSON.  We can use that information to assign further occurrences of the short form *Bourouiba* to also be PERSON.   \n",
    "\n",
    "Once we defined these two strategies, they can be combined in different ways. So, in Q5, you are asked to combine both strategies in a post-processing component for spaCy NER module.   Of course, using this text coherence will not work every time, and will unfortunately introduce some errors...  But let's try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the remainder of this section, we will be working with the seventh document in the corpus (index 6). Below we load the document and explore all entities within the document along with their corresponding NER type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document's text for the seventh document (index 6)\n",
    "doc = df[\"text\"][6]\n",
    "# Parse the text with spaCy\n",
    "doc_sp = sp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \"Coronavirus Brief\" is a ORG\n",
      "1: \"CBC\" is a ORG\n",
      "2: \"Canada\" is a GPE\n",
      "3: \"C.D. Howe\" is a PERSON\n",
      "4: \"Ontario\" is a GPE\n",
      "5: \"Monday\" is a DATE\n",
      "6: \"Alberta\" is a GPE\n",
      "7: \"first\" is a ORDINAL\n",
      "8: \"Saturday\" is a DATE\n",
      "9: \"Air Canada\" is a ORG\n",
      "10: \"Christmas\" is a DATE\n",
      "11: \"Canadians\" is a NORP\n",
      "12: \"more than $1.2 million\" is a MONEY\n",
      "13: \"England\" is a GPE\n",
      "14: \"Peter Cziborra/Reuters\" is a PERSON\n",
      "15: \"months\" is a DATE\n",
      "16: \"CBC\" is a ORG\n",
      "17: \"Andre Mayer\" is a PERSON\n",
      "18: \"Canada\" is a GPE\n",
      "19: \"19th-century\" is a DATE\n",
      "20: \"2013\" is a DATE\n",
      "21: \"Calgary\" is a GPE\n",
      "22: \"John Brown\" is a PERSON\n",
      "23: \"the University of Calgary\" is a ORG\n",
      "24: \"two-metre\" is a TIME\n",
      "25: \"Last week\" is a DATE\n",
      "26: \"Italian\" is a NORP\n",
      "27: \"Milan\" is a GPE\n",
      "28: \"35 kilometres\" is a QUANTITY\n",
      "29: \"Berlin\" is a GPE\n",
      "30: \"Budapest\" is a GPE\n",
      "31: \"Mexico City\" is a GPE\n",
      "32: \"Ahsan Habib\" is a PERSON\n",
      "33: \"Dalhousie University\" is a ORG\n",
      "34: \"U.S.\" is a GPE\n",
      "35: \"Atlanta\" is a GPE\n",
      "36: \"Chicago\" is a GPE\n",
      "37: \"Denver\" is a GPE\n",
      "38: \"Habib\" is a PERSON\n",
      "39: \"Brown\" is a PERSON\n",
      "40: \"Calgary\" is a GPE\n",
      "41: \"Housebrand\" is a ORG\n",
      "42: \"Zoom\" is a PERSON\n",
      "43: \"The National The At Issue\" is a WORK_OF_ART\n",
      "44: \"Quebec\" is a GPE\n",
      "45: \"Conservative\" is a NORP\n",
      "46: \"BRIEF Canada\" is a ORG\n",
      "47: \"C.D. Howe\" is a PERSON\n",
      "48: \"Canada\" is a GPE\n",
      "49: \"the C.D. Howe Institute's Business Cycle Council\" is a ORG\n",
      "50: \"today\" is a DATE\n",
      "51: \"Canada\" is a GPE\n",
      "52: \"February\" is a DATE\n",
      "53: \"one\" is a CARDINAL\n",
      "54: \"two\" is a CARDINAL\n",
      "55: \"three-month\" is a DATE\n",
      "56: \"two quarters\" is a DATE\n",
      "57: \"less than two months old\" is a DATE\n",
      "58: \"Canada\" is a GPE\n",
      "59: \"first\" is a ORDINAL\n",
      "60: \"Canada\" is a GPE\n",
      "61: \"2008\" is a DATE\n",
      "62: \"March\" is a DATE\n",
      "63: \"April\" is a DATE\n",
      "64: \"the entire month\" is a DATE\n",
      "65: \"Canada\" is a GPE\n",
      "66: \"Ontario\" is a GPE\n",
      "67: \"Monday\" is a DATE\n",
      "68: \"Alberta\" is a GPE\n",
      "69: \"Saturday\" is a DATE\n",
      "70: \"Ontario\" is a GPE\n",
      "71: \"today\" is a DATE\n",
      "72: \"Monday\" is a DATE\n",
      "73: \"Today\" is a DATE\n",
      "74: \"Doug Ford\" is a PERSON\n",
      "75: \"Alberta\" is a GPE\n",
      "76: \"Jason Kenney\" is a PERSON\n",
      "77: \"first\" is a ORDINAL\n",
      "78: \"Alberta\" is a GPE\n",
      "79: \"Saturday\" is a DATE\n",
      "80: \"mid-May\" is a DATE\n",
      "81: \"Kenney\" is a ORG\n",
      "82: \"Thursday\" is a DATE\n",
      "83: \"Monday\" is a DATE\n",
      "84: \"the end of the academic year\" is a DATE\n",
      "85: \"Kenney\" is a PERSON\n",
      "86: \"summer\" is a DATE\n",
      "87: \"Canada\" is a GPE\n",
      "88: \"Air Canada\" is a ORG\n",
      "89: \"Christmas\" is a DATE\n",
      "90: \"Air Canada\" is a ORG\n",
      "91: \"winter\" is a DATE\n",
      "92: \"Canadians\" is a NORP\n",
      "93: \"Tim Strauss\" is a PERSON\n",
      "94: \"Canadian\" is a NORP\n",
      "95: \"Air Canada\" is a ORG\n",
      "96: \"more than 90\" is a CARDINAL\n",
      "97: \"Canadian\" is a NORP\n",
      "98: \"today\" is a DATE\n",
      "99: \"North American\" is a NORP\n",
      "100: \"Air Canada\" is a ORG\n",
      "101: \"American Airlines\" is a ORG\n",
      "102: \"Transport Canada\" is a ORG\n",
      "103: \"two metres\" is a QUANTITY\n",
      "104: \"Air Canada\" is a ORG\n",
      "105: \"Christmas\" is a DATE\n",
      "106: \"Helane Becker\" is a PERSON\n",
      "107: \"first\" is a ORDINAL\n",
      "108: \"Canada\" is a GPE\n",
      "109: \"CBC News\" is a ORG\n",
      "110: \"covid@cbc.ca\" is a CARDINAL\n",
      "111: \"U.S.\" is a GPE\n",
      "112: \"U.S.\" is a GPE\n",
      "113: \"Anthony Fauci\" is a PERSON\n",
      "114: \"Canada\" is a GPE\n",
      "115: \"Health Canada\" is a ORG\n",
      "116: \"Health Canada\" is a ORG\n",
      "117: \"Gilead\" is a ORG\n",
      "118: \"CBC News\" is a ORG\n",
      "119: \"Gilead\" is a ORG\n",
      "120: \"Canada\" is a GPE\n",
      "121: \"Alberta\" is a GPE\n",
      "122: \"daily\" is a DATE\n",
      "123: \"Everly-Ann Toma\" is a FAC\n",
      "124: \"Grace Horsfall Couldwell\" is a ORG\n",
      "125: \"Paint\" is a ORG\n",
      "126: \"MJ\" is a PERSON\n",
      "127: \"Facebook\" is a ORG\n",
      "128: \"Alberta\" is a GPE\n",
      "129: \"MJ Stead\" is a PERSON\n",
      "130: \"Lori Toma\" is a PERSON\n",
      "131: \"Nancy Horsfall Couldwell\" is a PERSON\n",
      "132: \"Alta\" is a WORK_OF_ART\n",
      "133: \"MJ Stead\" is a PERSON\n",
      "134: \"Facebook\" is a ORG\n",
      "135: \"Paint\" is a ORG\n",
      "136: \"MJ\" is a ORG\n",
      "137: \"Facebook Live\" is a WORK_OF_ART\n",
      "138: \"20\" is a CARDINAL\n",
      "139: \"30\" is a CARDINAL\n",
      "140: \"more than 1,500\" is a CARDINAL\n",
      "141: \"more than one\" is a CARDINAL\n",
      "142: \"Two years ago\" is a DATE\n",
      "143: \"Stead\" is a PERSON\n",
      "144: \"100 days\" is a DATE\n",
      "145: \"Canada\" is a GPE\n",
      "146: \"covid@cbc.ca\" is a CARDINAL\n",
      "147: \"CBC News\" is a ORG\n",
      "148: \"daily\" is a DATE\n",
      "149: \"CBC News Network\" is a ORG\n",
      "150: \"CBC News Network\" is a ORG\n",
      "151: \"CBC\" is a ORG\n",
      "152: \"CBC News Network\" is a ORG\n",
      "153: \"CBC News\" is a ORG\n"
     ]
    }
   ],
   "source": [
    "# Display all entities from the text along with their index in the .ents iterator and the\n",
    "# corresponding NER type\n",
    "for i, token in enumerate(doc_sp.ents):\n",
    "    print(str(i) + \": \\\"\" + token.text + \"\\\" is a \" + token.label_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q2 - 3 marks**  \n",
    "As you can see in the results, sometimes the same entity was assigned different entity types (e.g. in document for Q1 *McGeer* was one time ORG, one time PERSON) since the NER algorithm looks sentence by sentence.  In the following function, the purpose will be to find all the possible entity types assigned to a single entity.\n",
    "\n",
    "Complete the definition of the *find_entity_types* function below. This function accepts as input a specific spaCy entity defined by the *entity* parameter (from the *.ents* iterable of entities) and a list of all spaCy entities defined by the *entities* parameter.     \n",
    "\n",
    "The function must find all entities of the same name as *entity* from *entities* (the same surface form). For each match between the entities, add the NER type of the entity from the list to the dictionary *type_counts* and track the number of times each NER type appears.     \n",
    "\n",
    "Ex: type_counts\\[NER type\\] = total number of times the count appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_types(entity, entities):\n",
    "    '''\n",
    "    Given a specific entity and a list of entities, finds all entities from the list that match the specified\n",
    "    entity, but are of a different type.\n",
    "    \n",
    "    Returns the different NER types that have been classified for an entity and the count per NER type\n",
    "    as a dictionary with the keys as the NER type and the value as the count\n",
    "    '''\n",
    "    type_counts = { }\n",
    "    per_count = 0\n",
    "    org_count = 0\n",
    "    gpe_count = 0\n",
    "    date_count = 0\n",
    "    cardinal_count = 0\n",
    "    norp_count =0\n",
    "    quantity_count = 0\n",
    "    for ent in entities:\n",
    "        if ent.text == entity.text:\n",
    "            if ent.label_ != entity.label_:\n",
    "                if ent.label_ =='PERSON':\n",
    "                    per_count += 1\n",
    "                elif ent.label_ =='ORG':\n",
    "                    org_count += 1\n",
    "                elif ent.label_ =='GPE':\n",
    "                    gpe_count += 1\n",
    "                elif ent.label_ =='CARDINAL':\n",
    "                    cardinal_count += 1   \n",
    "                elif ent.label_ =='DATE':\n",
    "                    date_count += 1 \n",
    "                elif ent.label_ =='NORP':\n",
    "                    norp_count += 1 \n",
    "                elif ent.label_ =='QUANTITY':\n",
    "                    quantity_count += 1     \n",
    "    type_counts['PERSON'] = per_count\n",
    "    type_counts['ORG'] = org_count\n",
    "    type_counts['GPE'] = gpe_count\n",
    "    type_counts['CARDINAL'] = cardinal_count\n",
    "    type_counts['DATE'] = date_count\n",
    "    type_counts['NORP'] = norp_count\n",
    "    type_counts['QUANTITY'] = quantity_count\n",
    "    return type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All possible NER types for \"Kenney\" are {'PERSON': 0, 'ORG': 1, 'GPE': 0, 'CARDINAL': 0, 'DATE': 0, 'NORP': 0, 'QUANTITY': 0}\n"
     ]
    }
   ],
   "source": [
    "# Test the above to find the result when checking for the types of the entity 'Kenney' \n",
    "# from the document loaded above\n",
    "print(\"All possible NER types for \\\"\" + doc_sp.ents[85].text + \"\\\" are \" + str(find_entity_types(doc_sp.ents[85], doc_sp.ents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q3 - 2 marks**  \n",
    "In the previous method, *find_entity_types*, we found all the possible entity types for a single entity.  Now, we want to use these to find the most common type.  If we look again at the results for Q1, in the case of *McGeer*, it's a tie.  But for *Bourouiba*, there is one GPE type, and 3 PERSON type, so the most common would be PERSON.   \n",
    "\n",
    "Complete the definition of the *most_common_type* function below. This function accepts as input a specific spaCy entity defined by the *entity* parameter (from the *.ents* iterable of entities) and a list of all spaCy entities defined by the *entities* parameter.        \n",
    "\n",
    "Note: You can handle ties as you please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_type(entity, entities):\n",
    "    '''\n",
    "    Given a specific entity and a list of entities, find the most similar entities and assign the\n",
    "    NER type to entity based on the most common NER type assigned to entities of the same name (if there\n",
    "    is a tie, you decide how to handle this).\n",
    "    \n",
    "    Returns the most common NER type based on similar entities\n",
    "    '''\n",
    "    type_counts = { }\n",
    "    per_count = 0\n",
    "    org_count = 0\n",
    "    gpe_count = 0\n",
    "    date_count = 0\n",
    "    cardinal_count = 0\n",
    "    norp_count =0\n",
    "    quantity_count = 0\n",
    "    for ent in entities:\n",
    "        if str(ent.text) == str(entity.text):\n",
    "                if ent.label_ =='PERSON':\n",
    "                    per_count += 1\n",
    "                elif ent.label_ =='ORG':\n",
    "                    org_count += 1\n",
    "                elif ent.label_ =='GPE':\n",
    "                    gpe_count += 1\n",
    "                elif ent.label_ =='CARDINAL':\n",
    "                    cardinal_count += 1   \n",
    "                elif ent.label_ =='DATE':\n",
    "                    date_count += 1 \n",
    "                elif ent.label_ =='NORP':\n",
    "                    norp_count += 1 \n",
    "                elif ent.label_ =='QUANTITY':\n",
    "                    quantity_count += 1\n",
    "    type_counts['PERSON'] = per_count\n",
    "    type_counts['ORG'] = org_count\n",
    "    type_counts['GPE'] = gpe_count\n",
    "    type_counts['CARDINAL'] = cardinal_count\n",
    "    type_counts['DATE'] = date_count\n",
    "    type_counts['NORP'] = norp_count\n",
    "    type_counts['QUANTITY'] = quantity_count\n",
    "    \n",
    "    max_value = max(type_counts.values())  # maximum value\n",
    "    max_keys = [k for k, v in type_counts.items() if v == max_value] \n",
    "    return max(max_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common NER type to \"Kenney\" is PERSON\n"
     ]
    }
   ],
   "source": [
    "# Test the above to find the result when checking for the types of the entity 'Kenney' \n",
    "# from the document loaded above\n",
    "print(\"The most common NER type to \\\"\" + doc_sp.ents[85].text + \"\\\" is \" + most_common_type(doc_sp.ents[85], doc_sp.ents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q4 - 2 marks**  \n",
    "Now we will work with a slightly more sophisticated method. We will once again work with the same *entity* and *entities* parameters, but this time you will need to assign *entity* the NER type of another entity in the *entities* iterator.    \n",
    "\n",
    "Specifically, you must look through *entities* to find a normalized form of *entity*. In this scenario, any entity that contains *entity* as a substring will be considered a valid selection for the normalized form (where the selected entity does not have the same name as *entity*). If a normalized form is found, return the NER type of that entity, the name of that entity, and the entity itself.    \n",
    "\n",
    "Ex: *CBC News Network* is the normalized form of *CBC*. Thus, if this entity is found, return the NER type from the entity (*ORG*) and the name of the entity (*CBC News Network*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_normalized_form(entity, entities):\n",
    "    '''\n",
    "    Given an entity and a list of entities, search the list of entities for any token that\n",
    "    is does not have the exact same text as entity and assign entity that token's NER type\n",
    "    if entity is a substring of that token.\n",
    "    \n",
    "    Returns the empty string if no normalized forms are found and the NER type of the normalized form if it is found.\n",
    "    Also returns the name of the entity found, if any (along with the entity).\n",
    "    '''\n",
    "    # MAY BE DONE SO THAT THE LAST GETS ADDED INSTEAD, THIS IS FINE.\n",
    "    # Recall to return the three requested components (NER type, the text, and the actual entity)\n",
    "    ner_type =''\n",
    "    ner_normal = ''\n",
    "    normal_entity = None\n",
    "    for ent in entities:\n",
    "        if (str(entity.text) in str(ent.text)) and (str(entity.text) != str(ent.text)) :\n",
    "            ner_type = str(ent.label_)\n",
    "            ner_normal = str(ent.text)\n",
    "            normal_entity= ent\n",
    "    return ner_type, ner_normal, normal_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PERSON', 'Jason Kenney', Jason Kenney)\n",
      "('ORG', 'CBC News Network', CBC News Network)\n"
     ]
    }
   ],
   "source": [
    "# Test the above to find the result when checking for the types of the entity 'Kenney' \n",
    "# from the document loaded above\n",
    "print(assign_normalized_form(doc_sp.ents[85], doc_sp.ents))\n",
    "# Test the above to find the result when checking for the types of the entity 'CBC News' \n",
    "# from the document loaded above\n",
    "print(assign_normalized_form(doc_sp.ents[153], doc_sp.ents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q5**  \n",
    "Now that you have defined several algorithms to perform NER disambiguation with text coherence, you will test your algorithms and use them to define a slightly more robust method of NER disambiguation by combining the techniques performed. You will then explore whether these techniques always help with NER disambiguation.       \n",
    "\n",
    "a) Revisit the document that was used in Q1 (index 1) and, for each entity, retrieve the normalized form of the entity (if any) and display only the normalized forms along with their NER types in the following format (only if there is a normalized form returned):    \n",
    "&emsp;*Original_entity refers to Normalized_entity, and is a NER_Type_of_Normalized_Form*    \n",
    "b) Define a more robust algorithm that combines the algorithms designed in the past few questions. This algorithm should accept a specific entity and list of entities as input, find the specific entity's normalized form (if any), and return an NER type for the normalized form based on the most common NER type for that entity. If no normalized form is found, the algorithm should continue by using the specific entity. You should also return the name of the normalized form (or of the original entity if there is no normalized form).       \n",
    "c) For the seventh document (index 6), run the algorithm defined in b) for each entity, printing the following for each entity:    \n",
    "&emsp;*Original_entity refers to Normalized_entity (if none, same as the original), and is a Most_common_NER_type_of_normalized_form*    \n",
    "d) Do any of the results found from performing NER disambiguation with text coherence Q5(c) seem problematic? Give an example of a problem that is occurring with our approaches and explain why this issue occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q5 (a) - 1 mark**     \n",
    "a) Revisit the document that was used in Q1 (index 1) and, for each entity, retrieve the normalized form of the entity (if any) and display only the normalized forms along with their NER types in the following format (only if there is a normalized form returned):    \n",
    "&emsp;*Original_entity refers to Normalized_entity, and is a NER_Type_of_Normalized_Form*    \n",
    "For example \"Bourouiba refers to Lydia Bourouiba, and is a PERSON\" would be printed for one entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select document 2\n",
    "doc = df[\"text\"][1]\n",
    "sp_doc_test = sp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two metres refers to farther than two metres and is a QUANTITY\n",
      "two refers to two-metre and is a QUANTITY\n",
      "two metres refers to farther than two metres and is a QUANTITY\n",
      "Wells refers to W.F. Wells and is a PERSON\n",
      "Bourouiba refers to Bourouiba/MIT/ and is a ORG\n",
      "Loeb refers to Mark Loeb and is a PERSON\n",
      "Loeb refers to Mark Loeb and is a PERSON\n",
      "McGeer refers to Allison McGeer and is a PERSON\n",
      "McGeer refers to Allison McGeer and is a PERSON\n",
      "2 refers to 2009 and is a DATE\n",
      "Bourouiba refers to Bourouiba/MIT/ and is a ORG\n",
      "Bourouiba refers to Bourouiba/MIT/ and is a ORG\n",
      "Bourouiba refers to Bourouiba/MIT/ and is a ORG\n",
      "two metres refers to farther than two metres and is a QUANTITY\n",
      "two refers to two-metre and is a QUANTITY\n",
      "two refers to two-metre and is a QUANTITY\n",
      "Second refers to Second Opinion and is a LAW\n",
      "McMaster refers to McMaster University and is a ORG\n",
      "Loeb refers to Mark Loeb and is a PERSON\n",
      "U.S. refers to U.S. National Institutes of Health and is a ORG\n",
      "U.S. National Institutes of Health refers to the U.S. National Institutes of Health and is a ORG\n",
      "Journal of the Royal Society Interface refers to Penn State University/Journal of the Royal Society Interface and is a ORG\n",
      "U.S. refers to U.S. National Institutes of Health and is a ORG\n"
     ]
    }
   ],
   "source": [
    "# TODO: Loop through and print the assigned phrase with the appropriate text\n",
    "# Example of the print statement structure (from document 1): Bourouiba refers to Lydia Bourouiba, and is a PERSON\n",
    "for ent in sp_doc_test.ents:\n",
    "        a,b,c= assign_normalized_form(ent, sp_doc_test.ents)\n",
    "        if b != '':\n",
    "            print(str(ent) + \" refers to \" + b + \" and is a \" + a )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q5 (b) - 2 marks**     \n",
    "b) Define a more robust algorithm that combines the algorithms designed in the past few questions. This algorithm should accept a specific entity and list of entities as input, find the specific entity's normalized form (if any), and return an NER type for the normalized form based on the most common NER type for that entity. If no normalized form is found, the algorithm should continue by using the specific entity. You should also return the name of the normalized form (or of the original entity if there is no normalized form).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_most_common_type(entity, entities):\n",
    "    '''\n",
    "    Determine the normalized form of an entity (if any; if none just use the entity) and\n",
    "    return the most frequent NER type for that normalized form from a list of entities.\n",
    "    '''\n",
    "    # TODO (Recall to return the name and the NER type that is found)\n",
    "    a,b,c = assign_normalized_form(entity, entities)\n",
    "    \n",
    "    return most_common_type(c, entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q5 (c) - 1 mark**     \n",
    "c) For the seventh document (index 6), run the algorithm defined in b) for each entity, printing the following for each entity:    \n",
    "&emsp;*Original_entity refers to Normalized_entity (if none, same as the original), and is a Most_common_NER_type_of_normalized_form*    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document's text\n",
    "doc = df[\"text\"][6]\n",
    "sp_doc_test = sp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBC refers to ORG and is a ORG\n",
      "Canada refers to ORG and is a ORG\n",
      "C.D. Howe refers to ORG and is a ORG\n",
      "months refers to DATE and is a DATE\n",
      "CBC refers to ORG and is a ORG\n",
      "Canada refers to ORG and is a ORG\n",
      "Calgary refers to ORG and is a ORG\n",
      "Habib refers to PERSON and is a PERSON\n",
      "Brown refers to PERSON and is a PERSON\n",
      "Calgary refers to ORG and is a ORG\n",
      "C.D. Howe refers to ORG and is a ORG\n",
      "Canada refers to ORG and is a ORG\n",
      "Canada refers to ORG and is a ORG\n",
      "one refers to CARDINAL and is a CARDINAL\n",
      "two refers to ORG and is a ORG\n",
      "Canada refers to ORG and is a ORG\n",
      "Canada refers to ORG and is a ORG\n",
      "Canada refers to ORG and is a ORG\n",
      "Kenney refers to PERSON and is a PERSON\n",
      "Kenney refers to PERSON and is a PERSON\n",
      "Canada refers to ORG and is a ORG\n",
      "Canadian refers to NORP and is a NORP\n",
      "Canadian refers to NORP and is a NORP\n",
      "Canada refers to ORG and is a ORG\n",
      "CBC News refers to ORG and is a ORG\n",
      "Canada refers to ORG and is a ORG\n",
      "CBC News refers to ORG and is a ORG\n",
      "Canada refers to ORG and is a ORG\n",
      "MJ refers to PERSON and is a PERSON\n",
      "Facebook refers to WORK_OF_ART and is a QUANTITY\n",
      "Facebook refers to WORK_OF_ART and is a QUANTITY\n",
      "MJ refers to PERSON and is a PERSON\n",
      "20 refers to DATE and is a DATE\n",
      "Stead refers to PERSON and is a PERSON\n",
      "Canada refers to ORG and is a ORG\n",
      "CBC News refers to ORG and is a ORG\n",
      "CBC refers to ORG and is a ORG\n",
      "CBC News refers to ORG and is a ORG\n"
     ]
    }
   ],
   "source": [
    "# TODO: Loop through and print the assigned phrase with the appropriate text\n",
    "for ent in sp_doc_test.ents:\n",
    "    a,b,c = assign_normalized_form(ent, sp_doc_test.ents) \n",
    "    if b !='':\n",
    "        print( str(ent) + \" refers to \" + a + \" and is a \" + normalized_most_common_type(ent, sp_doc_test.ents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q5 (d) - 2 marks**     \n",
    "d) Do any of the results found from performing NER disambiguation with text coherence Q5(c) seem problematic? Give an example of a problem that is occurring with our approaches and explain why this issue occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results that are found do not show the correct values for the tags that are not common such as law, this is because they were not included in the original most common type algorithm.\n",
    "It is also equating Canada as an ORG, this may be due to Canada in CBC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 2 - Entity Linking / Text enhancement**  \n",
    "\n",
    "For the second part of this notebook, we will be exploring how we can enhance the text of documents. In this scenario, we will be enhancing the text by performing entity linking. This means that we will attempt several methods of linking the entities that are detected by spaCy's NER to an active webpage that a reader can click on to obtain more information regarding the entity. Many websites, such as Wikipedia, perform Entity Linking to allow for more context to be obtained when reading a document.     \n",
    "\n",
    "Before going straight into an example through code, below is an example of how a text with no entity linking compares to a text with entity linking:    \n",
    "\n",
    "No entity linking:    \n",
    "During the pandemic, U.S. cities such as Atlanta, Chicago and Denver have made several adjustments to their transit systems.      \n",
    "\n",
    "With entity linking:    \n",
    "During the pandemic, U.S. cities such as <a href=\"http://en.wikipedia.org/wiki/Atlanta\">Atlanta</a>, <a href=\"http://en.wikipedia.org/wiki/Chicago\">Chicago</a> and <a href=\"http://en.wikipedia.org/wiki/Denver\">Denver</a> have made several adjustments to their transit systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you will be designing several methods of performing simple entity linking, below is an example that showcases how we can manually perform entity linking without any resources. This will showcase how it can be performed so that you will be able to use and create resources to create simple entity linking algorithms yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_example = \"During the pandemic, U.S. cities such as Atlanta, Chicago and Denver have made several adjustments to their transit systems\"\n",
    "# Parse the example sentence\n",
    "text_sp = sp(sentence_example)\n",
    "# This will store the enhanced version of the text\n",
    "enhanced_text = sentence_example\n",
    "# Loop through the entities that spaCy has found and replace them as needed to be in expanded form \n",
    "for token in text_sp.ents:\n",
    "    if token.text == \"Atlanta\":\n",
    "        enhanced_text = enhanced_text.replace(token.text, \"<a href=\\\"http://en.wikipedia.org/wiki/Atlanta\\\">Atlanta</a>\")\n",
    "    elif token.text == \"Chicago\":\n",
    "        enhanced_text = enhanced_text.replace(token.text, \"<a href=\\\"http://en.wikipedia.org/wiki/Chicago\\\">Chicago</a>\")\n",
    "    elif token.text == \"Denver\":\n",
    "        enhanced_text = enhanced_text.replace(token.text, \"<a href=\\\"http://en.wikipedia.org/wiki/Denver\\\">Denver</a>\")\n",
    "    \n",
    "# Write the result as an HTML file (open to view the enhanced text!)\n",
    "with open(\"enhanced_example.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(enhanced_text)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By opening the *enhanced_example.html* file that is now in the same directory as this notebook, you will be able to see\n",
    "that we linked the entities in the text.  \n",
    "\n",
    "That said, the process above is quite poor. It required manually stating which entities to work with and which URL to link to it. Thus, you will be answering questions for the rest of this section where you use and/or create resources that were manually put together to link entities in more general/robust methods. There are many different string matching techniques that can be used to help with entity linking, but we will stick with basic approaches for this notebook.   \n",
    "\n",
    "In the next question you will begin working with external resources. Thus, below we load the *US_Cities.csv* file to use to enhance the text with US cities in the following question. Note that each file contains two columns; *Text* and *URL*. *Text* refers to an entity name and *URL* refers to a corresponding *URL* that provides more information regarding the *Text*. The below example showcases how these files should be loaded and can be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York City - https://en.wikipedia.org/wiki/New_York_City\n",
      "Los Angelas - https://en.wikipedia.org/wiki/Los_Angeles\n",
      "Chicago - https://en.wikipedia.org/wiki/Chicago\n",
      "Houston - https://en.wikipedia.org/wiki/Houston\n",
      "Phoenix - https://en.wikipedia.org/wiki/Phoenix,_Arizona\n",
      "Philadelphia - https://en.wikipedia.org/wiki/Philadelphia\n",
      "San Antonio - https://en.wikipedia.org/wiki/San_Antonio\n",
      "San Diego - https://en.wikipedia.org/wiki/San_Diego\n",
      "Dallas - https://en.wikipedia.org/wiki/Dallas\n",
      "San Jose - https://en.wikipedia.org/wiki/San_Jose,_California\n",
      "Austin - https://en.wikipedia.org/wiki/Austin,_Texas\n",
      "Jacksonville - https://en.wikipedia.org/wiki/Jacksonville,_Florida\n",
      "Fort Worth - https://en.wikipedia.org/wiki/Fort_Worth,_Texas\n",
      "Columbus - https://en.wikipedia.org/wiki/Columbus,_Ohio\n",
      "Charlotte - https://en.wikipedia.org/wiki/Charlotte,_North_Carolina\n",
      "San Francisco - https://en.wikipedia.org/wiki/San_Francisco\n",
      "Indianapolis - https://en.wikipedia.org/wiki/Indianapolis\n",
      "Seattle - https://en.wikipedia.org/wiki/Seattle\n",
      "Denver - https://en.wikipedia.org/wiki/Denver\n",
      "Washington - https://en.wikipedia.org/wiki/Washington,_D.C.\n",
      "Boston - https://en.wikipedia.org/wiki/Boston\n",
      "El Paso - https://en.wikipedia.org/wiki/El_Paso,_Texas\n",
      "Nashville - https://en.wikipedia.org/wiki/Nashville,_Tennessee\n",
      "Detroit - https://en.wikipedia.org/wiki/Detroit\n",
      "Oklahoma City - https://en.wikipedia.org/wiki/Oklahoma_City\n",
      "Atlanta - https://en.wikipedia.org/wiki/Atlanta\n"
     ]
    }
   ],
   "source": [
    "# Start with the string match approach (exact match)\n",
    "# File content extracted from https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\n",
    "df_cities = pd.read_csv(\"US_Cities.csv\")\n",
    "# Print the Text and URL from each row, showcasing how to loop through the contents \n",
    "for i, row in df_cities.iterrows():\n",
    "    print(row[\"Text\"] + \" - \" + row[\"URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q6 - 3 marks**  \n",
    "Complete the *enhance_text_with_resource* function below. It receives the text from the document via *document_text*, the dataframe from the external resource to enhance the text with as *resource_df*, and the name of the file that you will output the results into (a .html file) as *filename*.   \n",
    "\n",
    "This function parses the text of the document and replaces any *entities* (.ents) found within the text with:    \n",
    "<a href=\\\"Some URL\">Entity text</a\\>     \n",
    "\n",
    "After enhancing the text with entity linking, we write the enhanced text into an html file and return the enhanced text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_text_with_resource(document_text, resource_df, filename):\n",
    "    '''\n",
    "    With a resource and document's text, enhance any entity found in the resource by linking the entity to\n",
    "    the appropriate webpage.\n",
    "    Write the file to the appropriate filename and return the enhanced text\n",
    "    '''\n",
    "    enhanced_text = document_text\n",
    "    # TODO: Parse the document with spaCy\n",
    "    doc_text_sp = sp(document_text)\n",
    "    # TODO: Go through the entities and edit the document's text accordingly\n",
    "    # Note: Be sure to not duplicate your enhancementes\n",
    "    replaced = []\n",
    "    for ent in doc_text_sp.ents:\n",
    "        for i in range(0,resource_df.shape[0],1):\n",
    "            if resource_df[\"Text\"][i] == str(ent.text) and str(ent.text) not in replaced:\n",
    "                enhanced_text = enhanced_text.replace(str(ent.text), \"<a href=\" +resource_df[\"URL\"][i]+ \">\" + str(ent.text)+\"</a>\")\n",
    "                replaced.append(ent.text)\n",
    "    # Write the result as an HTML file\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(enhanced_text)\n",
    "        f.close()\n",
    "    return enhanced_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q7 - 3 marks**  \n",
    "With the text enhancement algorithm designed (*enhance_text_with_resource*), you will now test the functionality when running the algorithm with three different resources. You will test the algorithm for each document already loaded in the code cell and run the algorithms with the following three resources:    \n",
    "1) A file containing several US cities: *US_Cities.csv*      \n",
    "2) A file containing all Provinces in Canada: *Canada_Provinces.csv*        \n",
    "3) A file containing several Canadian Universities: *Canada_Universities.csv*        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhance the text for the document below with the US cities\n",
    "doc = df[\"text\"][6]\n",
    "# TODO ...\n",
    "df_us_cities = pd.read_csv(\"US_Cities.csv\")\n",
    "uscity = enhance_text_with_resource(doc, df_us_cities, 'US_Cities.html')\n",
    "# Enhance the text for the document below with the Canadian provinces\n",
    "# File extracted from https://en.wikipedia.org/wiki/Provinces_and_territories_of_Canada\n",
    "doc = df[\"text\"][53]\n",
    "# TODO ...\n",
    "df_canada_provinces = pd.read_csv(\"Canada_Provinces.csv\")\n",
    "canprov = enhance_text_with_resource(doc, df_canada_provinces, 'Canada_Provinces.html')\n",
    "# Enhance the text for the document below with the Canadian universities\n",
    "# File extracted from https://en.wikipedia.org/wiki/List_of_universities_in_Canada\n",
    "doc = df[\"text\"][53]\n",
    "# TODO ...\n",
    "df_uni = pd.read_csv(\"Canada_Universities.csv\")\n",
    "uni = enhance_text_with_resource(doc, df_uni, 'Canada_Provinces.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you open the saved HTML files, you should find that the words that appear in the text and the resource now link directly to relevant information for that entity. We would also be able to enhance a document with many resources to link as many entities as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q8 - 2 marks**  \n",
    "Look through the enhanced texts that are generated by your tests in Q7. Do you see any universities that are not linked to when using the university resource? Why? Use the code cell below to output anything that you may need to investigate (if you have already noticed why this occurs earlier in the notebook, you do not need to investigate to find out why in the code cell) and answer the question in the markdown below that code cell.      \n",
    "\n",
    "Note: To find out, you should look through the .csv files and the text itself (both the initial text and spaCy's entity detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look through any outputs that may seem off to help understand why (if not already known)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO ...   \n",
    "The universities were not replaced because they have the names of other cities in them so spacy identifies them as either GPE or PERSON (carleton) and the whole term (ex \"University of Ottawa\") is not identified to be one entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q9 - 2 marks**  \n",
    "We will now combine some of the work done in Part 1 of this notebook with the work done in this part of the notebook. Specifically, we will perform NER type validation to ensure that when we enhance text with a resource that it only enhanced entities of the correct NER type. For example, when we use the resource of cities or provinces, ensure the entitiy that we are looking at is classified as a GPE before expanding it. The same concept applies for universities, which should be classified as an ORG.     \n",
    "\n",
    "Copy over your definition of the *enhance_text_with_resource* function, extend it to also accept an NER type as input (ex: *PERSON*, *ORG*, ...) and ensure that the text enhancement only occurs if *at least one entity with the same surface form* from the document contains the same NER type that was provided to the input parameter. This new function is named *enhance_text_with_resource_and_type*.    \n",
    "\n",
    "*NOTE (can ignore - just for more information):* In reality we would like to have it set such that only an entity of a specified type has its corresponding set of tokens within the text to be linked to the resource. However, this process can be tricky since the logic will involve creating flags within the text to know which entities have already been checked (ex: If *Nova Scotia* appears twice in the text, each instance with its own NER type, then we need to know the set of tokens that we are editing for each of the entities). Thus, you only need to ensure that if at least one entity of the same surface form contains the NER type and is in the text, then all instances of those entities are updated. If a resource contains the entities in the text, but they are all of a different type, ignore them. You are free to implement the more robust method detailed above if you like, but are recommended to perform the more simplistic approach that the question asks for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_text_with_resource_and_type(document_text, resource_df, filename, NER_type):\n",
    "    '''\n",
    "    With a NER type, a resource and document's text, enhance any entity found in the resource by linking the entity to\n",
    "    the appropriate webpage if at least one surface form contains the specified NER type.\n",
    "    Write the file to the appropriate filename and return the enhanced text\n",
    "    '''\n",
    "    enhanced_text = document_text\n",
    "    doc_text_sp = sp(document_text)\n",
    "    # TODO ...\n",
    "    replaced = []\n",
    "    for ent in doc_text_sp.ents:\n",
    "        for i in range(0,resource_df.shape[0],1):\n",
    "            if resource_df[\"Text\"][i] == str(ent.text) and str(ent.text) not in replaced and ent.label_ == NER_type:\n",
    "                enhanced_text = enhanced_text.replace(str(ent.text), \"<a href=\" +resource_df[\"URL\"][i]+ \">\" + str(ent.text)+\"</a>\")\n",
    "                replaced.append(ent.text)\n",
    "    \n",
    "    # Write the result as an HTML file\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(enhanced_text)\n",
    "        f.close()\n",
    "    return enhanced_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q10 - 2 marks**  \n",
    "Redo the tests performed in Q7 with the newly defined *enhance_text_with_resource* function. Ensure that you use the appropriate NER type depending on the resource being used for entitly linking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhance the text for the document below with the US cities\n",
    "doc = df[\"text\"][6]\n",
    "us_cities = enhance_text_with_resource_and_type(doc, df_us_cities, 'NER_US_Cities.html', \"GPE\")\n",
    "\n",
    "# Enhance the text for the document below with the Canadian provinces\n",
    "# File extracted from https://en.wikipedia.org/wiki/Provinces_and_territories_of_Canada\n",
    "doc = df[\"text\"][53]\n",
    "canprovs = enhance_text_with_resource_and_type(doc, df_canada_provinces, 'NER_Canada_Provinces.html', \"GPE\")\n",
    "\n",
    "\n",
    "# Enhance the text for the document below with the Canadian universities\n",
    "# File extracted from https://en.wikipedia.org/wiki/List_of_universities_in_Canada\n",
    "doc = df[\"text\"][53]\n",
    "unis = enhance_text_with_resource_and_type(doc, df_uni, 'NER_Canada_Universities.html', \"ORG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SIGNATURE:***\n",
    "My name is Shail Patel.\n",
    "My student number is 8234706\n",
    "I certify being the author of this assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
